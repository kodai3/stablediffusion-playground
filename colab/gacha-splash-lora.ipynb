{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe5f078",
   "metadata": {},
   "source": [
    "## Check you have GPU attached\n",
    "\n",
    "If you get error runing command. Go to `Edit > Notebook settings` and select \"GPU\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95616b43",
   "metadata": {},
   "source": [
    "## Download the lora file\n",
    "\n",
    "I download the file from civitai gacha-splash-lora\n",
    "\n",
    "https://civitai.com/models/13090/gacha-splash-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c73409",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://civitai.com/api/download/models/38884 --content-disposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74761e58",
   "metadata": {},
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf667dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install safetensors diffusers transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e386b82a",
   "metadata": {},
   "source": [
    "## Add function to convert safetensor to diffusers\n",
    "\n",
    "you cann't load safetensor file using `pipe.unet.load_attn_procs`, so convert to diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc1ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "def convert_lora_safetensor_to_diffusers(pipeline, checkpoint_path, LORA_PREFIX_UNET=\"lora_unet\", LORA_PREFIX_TEXT_ENCODER=\"lora_te\", alpha=0.75):\n",
    "    # load LoRA weight from .safetensors\n",
    "    state_dict = load_file(checkpoint_path)\n",
    "    visited = []\n",
    "    # directly update weight in diffusers model\n",
    "    for key in state_dict:\n",
    "        # it is suggested to print out the key, it usually will be something like below\n",
    "        # \"lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\"\n",
    "\n",
    "        # as we have set the alpha beforehand, so just skip\n",
    "        if \".alpha\" in key or key in visited:\n",
    "            continue\n",
    "\n",
    "        if \"text\" in key:\n",
    "            layer_infos = key.split(\".\")[0].split(LORA_PREFIX_TEXT_ENCODER + \"_\")[-1].split(\"_\")\n",
    "            curr_layer = pipeline.text_encoder\n",
    "        else:\n",
    "            layer_infos = key.split(\".\")[0].split(LORA_PREFIX_UNET + \"_\")[-1].split(\"_\")\n",
    "            curr_layer = pipeline.unet\n",
    "\n",
    "        # find the target layer\n",
    "        temp_name = layer_infos.pop(0)\n",
    "        while len(layer_infos) > -1:\n",
    "            try:\n",
    "                curr_layer = curr_layer.__getattr__(temp_name)\n",
    "                if len(layer_infos) > 0:\n",
    "                    temp_name = layer_infos.pop(0)\n",
    "                elif len(layer_infos) == 0:\n",
    "                    break\n",
    "            except Exception:\n",
    "                if len(temp_name) > 0:\n",
    "                    temp_name += \"_\" + layer_infos.pop(0)\n",
    "                else:\n",
    "                    temp_name = layer_infos.pop(0)\n",
    "\n",
    "        pair_keys = []\n",
    "        if \"lora_down\" in key:\n",
    "            pair_keys.append(key.replace(\"lora_down\", \"lora_up\"))\n",
    "            pair_keys.append(key)\n",
    "        else:\n",
    "            pair_keys.append(key)\n",
    "            pair_keys.append(key.replace(\"lora_up\", \"lora_down\"))\n",
    "\n",
    "        # update weight\n",
    "        if len(state_dict[pair_keys[0]].shape) == 4:\n",
    "            weight_up = state_dict[pair_keys[0]].squeeze(3).squeeze(2).to(torch.float32)\n",
    "            weight_down = state_dict[pair_keys[1]].squeeze(3).squeeze(2).to(torch.float32)\n",
    "            curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down).unsqueeze(2).unsqueeze(3)\n",
    "        else:\n",
    "            weight_up = state_dict[pair_keys[0]].to(torch.float32)\n",
    "            weight_down = state_dict[pair_keys[1]].to(torch.float32)\n",
    "            curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down)\n",
    "\n",
    "        # update visited list\n",
    "        for item in pair_keys:\n",
    "            visited.append(item)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d316e8",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f993827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "model_base = \"Linaqruf/anything-v3.0\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_base, torch_dtype=torch.float16)\n",
    "\n",
    "pipe = convert_lora_safetensor_to_diffusers(\n",
    "    pipe,\n",
    "    \"./gachaSplashLORA_v40.safetensors\",\n",
    ")\n",
    "\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "pipe.safety_checker = None if pipe.safety_checker is None else lambda images, **kwargs: (images, False)\n",
    "pipe.enable_attention_slicing()\n",
    "\n",
    "prompt = \"[(white background:1.5),::5] 1girl, mid shot, full body, ocean, sea waves, water splashes, sky, light particles, butterflies, night, starry sky <lora:lihui4JXK-b2-bf16-128-128-1-re1-ep3-768-DA-5015fix:1>\"\n",
    "negative_prompt = \"bad anatomy, extra fingers, watermark, (worst quality, low quality:1.4)\"\n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    height=728,\n",
    "    width=512,\n",
    "    num_inference_steps=24,\n",
    "    guidance_scale=7.5,\n",
    "    negative_prompt=negative_prompt,\n",
    ").images[0]\n",
    "\n",
    "image.save(\"./result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "filename = \"./result.png\"\n",
    "\n",
    "im = Image.open(filename)\n",
    "\n",
    "im"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
